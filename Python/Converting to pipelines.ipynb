{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion from top predictive model to pipelines\n",
    "\n",
    "I'll be using this script and seeing how easy it is to convert to the pipeline methodology.\n",
    "\n",
    "https://www.kaggle.com/aharless/xgboost-using-4th-quarter-for-validation/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.regression.quantile_regression import QuantReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runDetails = dict()\n",
    "modelDesc = dict()\n",
    "\n",
    "runDetails['MAKE_SUBMISSION'] = True          # Generate output file.\n",
    "runDetails['CV_ONLY'] = False                 # Do validation only; do not generate predicitons.\n",
    "runDetails['FIT_FULL_TRAIN_SET'] = True       # Fit model to full training set after doing validation.\n",
    "runDetails['FIT_2017_TRAIN_SET'] = False      # Use 2017 training data for full fit (no leak correction)\n",
    "runDetails['USE_SEASONAL_FEATURES'] = True\n",
    "runDetails['VAL_SPLIT_DATE'] = '2016-09-15'   # Cutoff date for validation split\n",
    "runDetails['FUDGE_FACTOR_SCALEDOWN'] = 0.3    # exponent to reduce optimized fudge factor for prediction\n",
    "runDetails['OPTIMIZE_FUDGE_FACTOR'] = True    # Optimize factor by which to multiply predictions.\n",
    "\n",
    "modelDesc['LEARNING_RATE'] = 0.007           # shrinkage rate for boosting roudns\n",
    "modelDesc['ROUNDS_PER_ETA'] = 20             # maximum number of boosting rounds times learning rate\n",
    "\n",
    "\n",
    "\n",
    "xgb_params = {  # best as of 2017-09-28 13:20 UTC\n",
    "    'eta': modelDesc['LEARNING_RATE'],\n",
    "    'max_depth': 7, \n",
    "    'subsample': 0.6,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 5.0,\n",
    "    'alpha': 0.65,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'base_score': y_mean,'taxdelinquencyyear'\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data and encoding objects to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('C:/Users/Evan/Documents/GitHub/Zillow/data')\n",
    "\n",
    "properties = pd.read_csv('properties_2016.csv', low_memory = False)\n",
    "properties17 = pd.read_csv('properties_2017.csv', low_memory = False)\n",
    "\n",
    "train = pd.read_csv(\"train_2016_v2.csv\", low_memory = False)\n",
    "for c in properties.columns:\n",
    "    properties[c]=properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating agg features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of properties in the zip\n",
    "zip_count = properties['regionidzip'].value_counts().to_dict()\n",
    "\n",
    "# Number of properties in the city\n",
    "city_count = properties['regionidcity'].value_counts().to_dict()\n",
    "\n",
    "# Median year of construction by neighborhood\n",
    "medyear = properties.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n",
    "\n",
    "# Mean square feet by neighborhood\n",
    "meanarea = properties.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n",
    "\n",
    "# Neighborhood latitude and longitude\n",
    "medlat = properties.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n",
    "medlong = properties.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "select_qtr4 = pd.to_datetime(train_df[\"transactiondate\"]) >= runDetails['VAL_SPLIT_DATE']\n",
    "if runDetails['USE_SEASONAL_FEATURES']:\n",
    "    basedate = pd.to_datetime('2015-11-15').toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1169"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs to features that depend on target variable\n",
    "# (Ideally these should be recalculated, and the dependent features recalculated,\n",
    "#  when fitting to the full training set.  But I haven't implemented that yet.)\n",
    "\n",
    "# Standard deviation of target value for properties in the city/zip/neighborhood\n",
    "citystd = train_df[~select_qtr4].groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n",
    "zipstd = train_df[~select_qtr4].groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n",
    "hoodstd = train_df[~select_qtr4].groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_features(df):\n",
    "    \n",
    "    # Nikunj's features\n",
    "    # Number of properties in the zip\n",
    "    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n",
    "    # Number of properties in the city\n",
    "    df['N-city_count'] = df['regionidcity'].map(city_count)\n",
    "    # Does property have a garage, pool or hot tub and AC?\n",
    "    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n",
    "                         (df['pooltypeid10']>0) & \\\n",
    "                         (df['airconditioningtypeid']!=5))*1 \n",
    "\n",
    "    # More features\n",
    "    # Mean square feet of neighborhood properties\n",
    "    df['mean_area'] = df['regionidneighborhood'].map(meanarea)\n",
    "    # Median year of construction of neighborhood properties\n",
    "    df['med_year'] = df['regionidneighborhood'].map(medyear)\n",
    "    # Neighborhood latitude and longitude\n",
    "    df['med_lat'] = df['regionidneighborhood'].map(medlat)\n",
    "    df['med_long'] = df['regionidneighborhood'].map(medlong)\n",
    "\n",
    "    df['zip_std'] = df['regionidzip'].map(zipstd)\n",
    "    df['city_std'] = df['regionidcity'].map(citystd)\n",
    "    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)\n",
    "    \n",
    "    if runDetails['USE_SEASONAL_FEATURES']:\n",
    "        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "                             (2*np.pi/365.25) ).apply(np.cos)\n",
    "        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "                             (2*np.pi/365.25) ).apply(np.sin)\n",
    "        \n",
    "    return(df)\n",
    "        \n",
    "def calculate_features_new(df):\n",
    "    \n",
    "    # Nikunj's features\n",
    "    # Number of properties in the zip\n",
    "    df['N-zip_count'] = df['regionidzip'].agg('count')\n",
    "    # Number of properties in the city\n",
    "    df['N-city_count'] = df['regionidcity'].agg('count')\n",
    "    # Does property have a garage, pool or hot tub and AC?\n",
    "    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n",
    "                         (df['pooltypeid10']>0) & \\\n",
    "                         (df['airconditioningtypeid']!=5))*1 \n",
    "\n",
    "    # More features\n",
    "    # Mean square feet of neighborhood properties\n",
    "    df['mean_area'] = df['regionidneighborhood'].agg('median')\n",
    "    # Median year of construction of neighborhood properties\n",
    "    df['med_year'] = df['regionidneighborhood'].agg('median')\n",
    "    # Neighborhood latitude and longitude\n",
    "    df['med_lat'] = df['regionidneighborhood'].agg('median')\n",
    "    df['med_long'] = df['regionidneighborhood'].agg('median')\n",
    "\n",
    "    df['zip_std'] = df['regionidzip'].map(zipstd)\n",
    "    df['city_std'] = df['regionidcity'].map(citystd)\n",
    "    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)\n",
    "    \n",
    "    if runDetails['USE_SEASONAL_FEATURES']:\n",
    "        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "                             (2*np.pi/365.25) ).apply(np.cos)\n",
    "        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "                             (2*np.pi/365.25) ).apply(np.sin)  \n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = calculate_features(train_df)\n",
    "dfTest = calculate_features_new(train_df)\n",
    "\n",
    "df.equals(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropvars = ['parcelid', 'airconditioningtypeid', 'buildingclasstypeid',\n",
    "            'buildingqualitytypeid', 'regionidcity']\n",
    "droptrain = ['logerror', 'transactiondate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape full training set: (90275, 72)\n",
      "Dropped vars: 7\n",
      "Shape valid X: (14304, 65)\n",
      "Shape valid y: (14304,)\n",
      "\n",
      "Full training set after removing outliers, before dropping vars:\n",
      "Shape training set: (88528, 72)\n",
      "\n",
      "Training subset after removing outliers:\n",
      "Shape train X: (74478, 65)\n",
      "Shape train y: (74478,)\n",
      "\n",
      "Full trainng set:\n",
      "Shape train X: (74478, 65)\n",
      "Shape train y: (74478,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "calculate_features(train_df)\n",
    "\n",
    "x_valid = train_df.drop(dropvars+droptrain, axis=1)[select_qtr4]\n",
    "y_valid = train_df[\"logerror\"].values.astype(np.float32)[select_qtr4]\n",
    "\n",
    "print('Shape full training set: {}'.format(train_df.shape))\n",
    "print('Dropped vars: {}'.format(len(dropvars+droptrain)))\n",
    "print('Shape valid X: {}'.format(x_valid.shape))\n",
    "print('Shape valid y: {}'.format(y_valid.shape))\n",
    "\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "print('\\nFull training set after removing outliers, before dropping vars:')     \n",
    "print('Shape training set: {}\\n'.format(train_df.shape))\n",
    "\n",
    "if runDetails['FIT_FULL_TRAIN_SET']:\n",
    "    full_train = train_df.copy()\n",
    "\n",
    "train_df=train_df[~select_qtr4]\n",
    "x_train=train_df.drop(dropvars+droptrain, axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "n_train = x_train.shape[0]\n",
    "print('Training subset after removing outliers:')     \n",
    "print('Shape train X: {}'.format(x_train.shape))\n",
    "print('Shape train y: {}'.format(y_train.shape))\n",
    "\n",
    "if runDetails['FIT_FULL_TRAIN_SET']:\n",
    "    x_full = full_train.drop(dropvars+droptrain, axis=1)\n",
    "    y_full = full_train[\"logerror\"].values.astype(np.float32)\n",
    "    n_full = x_full.shape[0]\n",
    "    print('\\nFull trainng set:')     \n",
    "    print('Shape train X: {}'.format(x_train.shape))\n",
    "    print('Shape train y: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test: (2985217, 65)\n"
     ]
    }
   ],
   "source": [
    "if not runDetails['CV_ONLY']:\n",
    "    test_df = properties.copy()\n",
    "    droptest = []\n",
    "    if runDetails['USE_SEASONAL_FEATURES']:\n",
    "        test_df['transactiondate'] = '2016-10-31'\n",
    "        droptest = ['transactiondate']\n",
    "    calculate_features(test_df)\n",
    "    x_test = test_df.drop(dropvars+droptest, axis=1)\n",
    "    print('Shape test: {}'.format(x_test.shape))\n",
    "\n",
    "    # Process properties for 2017\n",
    "    properties = properties17\n",
    "    for c in properties.columns:\n",
    "        properties[c]=properties[c].fillna(-1)\n",
    "        if properties[c].dtype == 'object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(properties[c].values))\n",
    "            properties[c] = lbl.transform(list(properties[c].values))\n",
    "    zip_count = properties['regionidzip'].value_counts().to_dict()\n",
    "    city_count = properties['regionidcity'].value_counts().to_dict()\n",
    "    medyear = properties.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n",
    "    meanarea = properties.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n",
    "    medlat = properties.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n",
    "    medlong = properties.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n",
    "\n",
    "    test_df = properties.copy()\n",
    "    if runDetails['USE_SEASONAL_FEATURES']:\n",
    "        test_df['transactiondate'] = '2017-10-31'\n",
    "    calculate_features(test_df)\n",
    "    x_test17 = test_df.drop(dropvars+droptest, axis=1)   \n",
    "    del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df\n",
    "del select_qtr4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data into a xgboost friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dvalid_x = xgb.DMatrix(x_valid)\n",
    "dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n",
    "if not runDetails['CV_ONLY']:\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "    dtest17 = xgb.DMatrix(x_test17)\n",
    "    del x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting rounds: 2857\n",
      "Early stoping rounds: 143\n"
     ]
    }
   ],
   "source": [
    "num_boost_rounds = round( modelDesc['ROUNDS_PER_ETA'] / xgb_params['eta'] )\n",
    "early_stopping_rounds = round( num_boost_rounds / 20 )\n",
    "print('Boosting rounds: {}'.format(num_boost_rounds))\n",
    "print('Early stoping rounds: {}'.format(early_stopping_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.053445\teval-mae:0.065273\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 143 rounds.\n",
      "[10]\ttrain-mae:0.053357\teval-mae:0.065206\n",
      "[20]\ttrain-mae:0.053269\teval-mae:0.065138\n",
      "[30]\ttrain-mae:0.053188\teval-mae:0.065083\n",
      "[40]\ttrain-mae:0.053106\teval-mae:0.065026\n",
      "[50]\ttrain-mae:0.053027\teval-mae:0.064976\n",
      "[60]\ttrain-mae:0.052953\teval-mae:0.064925\n",
      "[70]\ttrain-mae:0.052887\teval-mae:0.064881\n",
      "[80]\ttrain-mae:0.052826\teval-mae:0.064841\n",
      "[90]\ttrain-mae:0.052769\teval-mae:0.064803\n",
      "[100]\ttrain-mae:0.052722\teval-mae:0.064775\n",
      "[110]\ttrain-mae:0.052665\teval-mae:0.064742\n",
      "[120]\ttrain-mae:0.052614\teval-mae:0.064714\n",
      "[130]\ttrain-mae:0.052567\teval-mae:0.064689\n",
      "[140]\ttrain-mae:0.052523\teval-mae:0.064663\n",
      "[150]\ttrain-mae:0.052475\teval-mae:0.064637\n",
      "[160]\ttrain-mae:0.052428\teval-mae:0.064613\n",
      "[170]\ttrain-mae:0.052383\teval-mae:0.064592\n",
      "[180]\ttrain-mae:0.052342\teval-mae:0.064572\n",
      "[190]\ttrain-mae:0.052307\teval-mae:0.064555\n",
      "[200]\ttrain-mae:0.052269\teval-mae:0.064542\n",
      "[210]\ttrain-mae:0.052235\teval-mae:0.064529\n",
      "[220]\ttrain-mae:0.052199\teval-mae:0.064515\n",
      "[230]\ttrain-mae:0.052162\teval-mae:0.064501\n",
      "[240]\ttrain-mae:0.05213\teval-mae:0.064488\n",
      "[250]\ttrain-mae:0.052095\teval-mae:0.064475\n",
      "[260]\ttrain-mae:0.052061\teval-mae:0.064459\n",
      "[270]\ttrain-mae:0.052027\teval-mae:0.064447\n",
      "[280]\ttrain-mae:0.051994\teval-mae:0.064433\n",
      "[290]\ttrain-mae:0.051967\teval-mae:0.064425\n",
      "[300]\ttrain-mae:0.051934\teval-mae:0.064416\n",
      "[310]\ttrain-mae:0.05191\teval-mae:0.064407\n",
      "[320]\ttrain-mae:0.051886\teval-mae:0.064401\n",
      "[330]\ttrain-mae:0.051858\teval-mae:0.064394\n",
      "[340]\ttrain-mae:0.051835\teval-mae:0.064385\n",
      "[350]\ttrain-mae:0.051809\teval-mae:0.064377\n",
      "[360]\ttrain-mae:0.051784\teval-mae:0.064372\n",
      "[370]\ttrain-mae:0.051757\teval-mae:0.064363\n",
      "[380]\ttrain-mae:0.05173\teval-mae:0.064359\n",
      "[390]\ttrain-mae:0.051706\teval-mae:0.064355\n",
      "[400]\ttrain-mae:0.051687\teval-mae:0.064353\n",
      "[410]\ttrain-mae:0.05166\teval-mae:0.064347\n",
      "[420]\ttrain-mae:0.051634\teval-mae:0.064343\n",
      "[430]\ttrain-mae:0.051605\teval-mae:0.064336\n",
      "[440]\ttrain-mae:0.051582\teval-mae:0.064331\n",
      "[450]\ttrain-mae:0.051557\teval-mae:0.06433\n",
      "[460]\ttrain-mae:0.05154\teval-mae:0.064328\n",
      "[470]\ttrain-mae:0.051519\teval-mae:0.064325\n",
      "[480]\ttrain-mae:0.051497\teval-mae:0.064321\n",
      "[490]\ttrain-mae:0.051476\teval-mae:0.064317\n",
      "[500]\ttrain-mae:0.051454\teval-mae:0.064314\n",
      "[510]\ttrain-mae:0.051432\teval-mae:0.064312\n",
      "[520]\ttrain-mae:0.051411\teval-mae:0.064311\n",
      "[530]\ttrain-mae:0.051389\teval-mae:0.064307\n",
      "[540]\ttrain-mae:0.051366\teval-mae:0.064304\n",
      "[550]\ttrain-mae:0.051343\teval-mae:0.0643\n",
      "[560]\ttrain-mae:0.051323\teval-mae:0.064295\n",
      "[570]\ttrain-mae:0.0513\teval-mae:0.064291\n",
      "[580]\ttrain-mae:0.051279\teval-mae:0.064288\n",
      "[590]\ttrain-mae:0.051259\teval-mae:0.064286\n",
      "[600]\ttrain-mae:0.051238\teval-mae:0.064283\n",
      "[610]\ttrain-mae:0.051221\teval-mae:0.064285\n",
      "[620]\ttrain-mae:0.051202\teval-mae:0.064282\n",
      "[630]\ttrain-mae:0.05118\teval-mae:0.064279\n",
      "[640]\ttrain-mae:0.05116\teval-mae:0.064276\n",
      "[650]\ttrain-mae:0.05114\teval-mae:0.064274\n",
      "[660]\ttrain-mae:0.05112\teval-mae:0.064274\n",
      "[670]\ttrain-mae:0.051102\teval-mae:0.064274\n",
      "[680]\ttrain-mae:0.051084\teval-mae:0.064273\n",
      "[690]\ttrain-mae:0.051066\teval-mae:0.064274\n",
      "[700]\ttrain-mae:0.051049\teval-mae:0.064273\n",
      "[710]\ttrain-mae:0.05103\teval-mae:0.064271\n",
      "[720]\ttrain-mae:0.051014\teval-mae:0.064272\n",
      "[730]\ttrain-mae:0.050993\teval-mae:0.064269\n",
      "[740]\ttrain-mae:0.050975\teval-mae:0.064269\n",
      "[750]\ttrain-mae:0.050956\teval-mae:0.064268\n",
      "[760]\ttrain-mae:0.050934\teval-mae:0.064266\n",
      "[770]\ttrain-mae:0.050917\teval-mae:0.064265\n",
      "[780]\ttrain-mae:0.050899\teval-mae:0.064262\n",
      "[790]\ttrain-mae:0.050877\teval-mae:0.06426\n",
      "[800]\ttrain-mae:0.050861\teval-mae:0.064261\n",
      "[810]\ttrain-mae:0.050841\teval-mae:0.064258\n",
      "[820]\ttrain-mae:0.050821\teval-mae:0.064259\n",
      "[830]\ttrain-mae:0.050803\teval-mae:0.064259\n",
      "[840]\ttrain-mae:0.050786\teval-mae:0.064262\n",
      "[850]\ttrain-mae:0.050769\teval-mae:0.064263\n",
      "[860]\ttrain-mae:0.050752\teval-mae:0.064266\n",
      "[870]\ttrain-mae:0.050738\teval-mae:0.064266\n",
      "[880]\ttrain-mae:0.050723\teval-mae:0.064265\n",
      "[890]\ttrain-mae:0.050708\teval-mae:0.064267\n",
      "[900]\ttrain-mae:0.050691\teval-mae:0.064266\n",
      "[910]\ttrain-mae:0.050676\teval-mae:0.064268\n",
      "[920]\ttrain-mae:0.050657\teval-mae:0.064267\n",
      "[930]\ttrain-mae:0.050639\teval-mae:0.064267\n",
      "[940]\ttrain-mae:0.050622\teval-mae:0.064268\n",
      "Stopping. Best iteration:\n",
      "[804]\ttrain-mae:0.050853\teval-mae:0.064258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n",
    "model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n",
    "                  evals=evals, early_stopping_rounds=early_stopping_rounds, \n",
    "                  verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost validation set predictions:\n",
      "          0\n",
      "0  0.001607\n",
      "1  0.022175\n",
      "2  0.020656\n",
      "3  0.012385\n",
      "4  0.019222\n",
      "\n",
      "Mean absolute validation error:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.064258203"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n",
    "print( \"XGBoost validation set predictions:\" )\n",
    "print( pd.DataFrame(valid_pred).head() )\n",
    "print(\"\\nMean absolute validation error:\")\n",
    "mean_absolute_error(y_valid, valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAD Fit for Fudge Factor:\n",
      "                         QuantReg Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Pseudo R-squared:              0.01331\n",
      "Model:                       QuantReg   Bandwidth:                    0.009325\n",
      "Method:                 Least Squares   Sparsity:                       0.1024\n",
      "Date:                Sat, 07 Oct 2017   No. Observations:                14304\n",
      "Time:                        15:18:38   Df Residuals:                    14303\n",
      "                                        Df Model:                            1\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.9598      0.027     35.599      0.000       0.907       1.013\n",
      "==============================================================================\n",
      "Optimized fudge factor: 0.959837\n",
      "\n",
      "Mean absolute validation error with optimized fudge factor: \n",
      "0.064255\n",
      "Scaled down fudge factor: 0.987777771561\n",
      "\n",
      "Mean absolute validation error with scaled down fudge factor: \n",
      "0.0642567\n"
     ]
    }
   ],
   "source": [
    "if runDetails['OPTIMIZE_FUDGE_FACTOR']:\n",
    "    mod = QuantReg(y_valid, valid_pred)\n",
    "    res = mod.fit(q=.5)\n",
    "    print(\"\\nLAD Fit for Fudge Factor:\")\n",
    "    print(res.summary())\n",
    "\n",
    "    fudge = res.params[0]\n",
    "    print(\"Optimized fudge factor:\", fudge)\n",
    "    print(\"\\nMean absolute validation error with optimized fudge factor: \")\n",
    "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n",
    "\n",
    "    fudge **= runDetails['FUDGE_FACTOR_SCALEDOWN']\n",
    "    print(\"Scaled down fudge factor:\", fudge)\n",
    "    print(\"\\nMean absolute validation error with scaled down fudge factor: \")\n",
    "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n",
    "else:\n",
    "    fudge=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.053266\n",
      "[10]\ttrain-mae:0.053176\n",
      "[20]\ttrain-mae:0.053092\n",
      "[30]\ttrain-mae:0.053007\n",
      "[40]\ttrain-mae:0.052932\n",
      "[50]\ttrain-mae:0.052855\n",
      "[60]\ttrain-mae:0.052788\n",
      "[70]\ttrain-mae:0.052725\n",
      "[80]\ttrain-mae:0.052671\n",
      "[90]\ttrain-mae:0.052615\n",
      "[100]\ttrain-mae:0.052562\n",
      "[110]\ttrain-mae:0.05251\n",
      "[120]\ttrain-mae:0.052461\n",
      "[130]\ttrain-mae:0.052414\n",
      "[140]\ttrain-mae:0.052365\n",
      "[150]\ttrain-mae:0.052319\n",
      "[160]\ttrain-mae:0.05228\n",
      "[170]\ttrain-mae:0.052237\n",
      "[180]\ttrain-mae:0.052199\n",
      "[190]\ttrain-mae:0.052164\n",
      "[200]\ttrain-mae:0.052128\n",
      "[210]\ttrain-mae:0.052095\n",
      "[220]\ttrain-mae:0.052058\n",
      "[230]\ttrain-mae:0.052024\n",
      "[240]\ttrain-mae:0.051993\n",
      "[250]\ttrain-mae:0.051962\n",
      "[260]\ttrain-mae:0.051933\n",
      "[270]\ttrain-mae:0.051905\n",
      "[280]\ttrain-mae:0.051878\n",
      "[290]\ttrain-mae:0.051849\n",
      "[300]\ttrain-mae:0.051818\n",
      "[310]\ttrain-mae:0.051792\n",
      "[320]\ttrain-mae:0.051766\n",
      "[330]\ttrain-mae:0.051741\n",
      "[340]\ttrain-mae:0.051716\n",
      "[350]\ttrain-mae:0.051689\n",
      "[360]\ttrain-mae:0.051663\n",
      "[370]\ttrain-mae:0.051638\n",
      "[380]\ttrain-mae:0.051614\n",
      "[390]\ttrain-mae:0.051592\n",
      "[400]\ttrain-mae:0.051569\n",
      "[410]\ttrain-mae:0.051546\n",
      "[420]\ttrain-mae:0.05152\n",
      "[430]\ttrain-mae:0.051501\n",
      "[440]\ttrain-mae:0.051478\n",
      "[450]\ttrain-mae:0.051459\n",
      "[460]\ttrain-mae:0.051437\n",
      "[470]\ttrain-mae:0.051421\n",
      "[480]\ttrain-mae:0.051399\n",
      "[490]\ttrain-mae:0.051379\n",
      "[500]\ttrain-mae:0.051355\n",
      "[510]\ttrain-mae:0.051338\n",
      "[520]\ttrain-mae:0.051319\n",
      "[530]\ttrain-mae:0.0513\n",
      "[540]\ttrain-mae:0.051281\n",
      "[550]\ttrain-mae:0.051266\n",
      "[560]\ttrain-mae:0.051247\n",
      "[570]\ttrain-mae:0.05123\n",
      "[580]\ttrain-mae:0.051212\n",
      "[590]\ttrain-mae:0.051191\n",
      "[600]\ttrain-mae:0.05117\n",
      "[610]\ttrain-mae:0.051155\n",
      "[620]\ttrain-mae:0.051139\n",
      "[630]\ttrain-mae:0.051121\n",
      "[640]\ttrain-mae:0.0511\n",
      "[650]\ttrain-mae:0.051082\n",
      "[660]\ttrain-mae:0.051065\n",
      "[670]\ttrain-mae:0.051049\n",
      "[680]\ttrain-mae:0.051033\n",
      "[690]\ttrain-mae:0.051019\n",
      "[700]\ttrain-mae:0.051001\n",
      "[710]\ttrain-mae:0.050985\n",
      "[720]\ttrain-mae:0.050967\n",
      "[730]\ttrain-mae:0.050952\n",
      "[740]\ttrain-mae:0.050933\n",
      "[750]\ttrain-mae:0.050918\n",
      "[760]\ttrain-mae:0.050902\n",
      "[770]\ttrain-mae:0.050884\n",
      "[780]\ttrain-mae:0.050868\n",
      "[790]\ttrain-mae:0.050852\n",
      "[800]\ttrain-mae:0.050838\n",
      "[810]\ttrain-mae:0.050823\n",
      "[820]\ttrain-mae:0.050807\n",
      "[830]\ttrain-mae:0.050791\n",
      "[840]\ttrain-mae:0.050777\n",
      "[850]\ttrain-mae:0.050759\n",
      "[860]\ttrain-mae:0.050744\n",
      "[870]\ttrain-mae:0.050728\n",
      "[880]\ttrain-mae:0.050714\n",
      "[890]\ttrain-mae:0.050698\n",
      "[900]\ttrain-mae:0.050684\n",
      "[910]\ttrain-mae:0.05067\n",
      "[920]\ttrain-mae:0.050655\n",
      "[930]\ttrain-mae:0.050641\n",
      "[940]\ttrain-mae:0.050627\n",
      "[950]\ttrain-mae:0.050612\n"
     ]
    }
   ],
   "source": [
    "if runDetails['FIT_FULL_TRAIN_SET'] and not runDetails['CV_ONLY']:\n",
    "    if runDetails['FIT_2017_TRAIN_SET']:\n",
    "        \n",
    "        train = pd.read_csv('train_2017.csv')\n",
    "        train_df = train.merge(properties, how='left', on='parcelid')\n",
    "        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n",
    "        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n",
    "        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n",
    "        calculate_features(train_df)\n",
    "        x_full = train_df.drop(dropvars+droptrain, axis=1)\n",
    "        y_full = train_df[\"logerror\"].values.astype(np.float32)\n",
    "        n_full = x_full.shape[0]     \n",
    "        \n",
    "    dtrain = xgb.DMatrix(x_full, y_full)\n",
    "    num_boost_rounds = int(model.best_ntree_limit*n_full/n_train)\n",
    "    full_model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds, \n",
    "                           evals=[(dtrain,'train')], verbose_eval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
