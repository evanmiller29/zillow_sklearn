{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion from top predictive model to pipelines\n",
    "\n",
    "I'll be using this script and seeing how easy it is to convert to the pipeline methodology.\n",
    "\n",
    "https://www.kaggle.com/aharless/xgboost-using-4th-quarter-for-validation/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Evan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.regression.quantile_regression import QuantReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "runDetails = dict()\n",
    "modelDesc = dict()\n",
    "\n",
    "runDetails['MAKE_SUBMISSION'] = True          # Generate output file.\n",
    "runDetails['CV_ONLY'] = False                 # Do validation only; do not generate predicitons.\n",
    "runDetails['FIT_FULL_TRAIN_SET'] = True       # Fit model to full training set after doing validation.\n",
    "runDetails['FIT_2017_TRAIN_SET'] = False      # Use 2017 training data for full fit (no leak correction)\n",
    "runDetails['USE_SEASONAL_FEATURES'] = True\n",
    "runDetails['VAL_SPLIT_DATE'] = '2016-09-15'   # Cutoff date for validation split\n",
    "runDetails['FUDGE_FACTOR_SCALEDOWN'] = 0.3    # exponent to reduce optimized fudge factor for prediction\n",
    "runDetails['OPTIMIZE_FUDGE_FACTOR'] = True    # Optimize factor by which to multiply predictions.\n",
    "\n",
    "modelDesc['LEARNING_RATE'] = 0.007           # shrinkage rate for boosting roudns\n",
    "modelDesc['ROUNDS_PER_ETA'] = 20             # maximum number of boosting rounds times learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data and encoding objects to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('C:/Users/Evan/Documents/GitHub/Zillow/data')\n",
    "\n",
    "properties = pd.read_csv('properties_2016.csv', low_memory = False)\n",
    "properties17 = pd.read_csv('properties_2017.csv', low_memory = False)\n",
    "\n",
    "train = pd.read_csv(\"train_2016_v2.csv\", low_memory = False)\n",
    "\n",
    "for c in properties.columns:\n",
    "    properties[c]=properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7931b5040355>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'parcelid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mselect_qtr4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"transactiondate\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mrunDetails\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'VAL_SPLIT_DATE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrunDetails\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'USE_SEASONAL_FEATURES'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbasedate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2015-11-15'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoordinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "select_qtr4 = pd.to_datetime(train_df[\"transactiondate\"]) >= runDetails['VAL_SPLIT_DATE']\n",
    "if runDetails['USE_SEASONAL_FEATURES']:\n",
    "    basedate = pd.to_datetime('2015-11-15').toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating agg features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs to features that depend on target variable\n",
    "# (Ideally these should be recalculated, and the dependent features recalculated,\n",
    "#  when fitting to the full training set.  But I haven't implemented that yet.)\n",
    "\n",
    "# Standard deviation of target value for properties in the city/zip/neighborhood\n",
    "citystd = train_df[~select_qtr4].groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n",
    "zipstd = train_df[~select_qtr4].groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n",
    "hoodstd = train_df[~select_qtr4].groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(df):\n",
    "    \n",
    "    # Nikunj's features\n",
    "    # Number of properties in the zip\n",
    "    df['N-zip_count'] = df['regionidzip'].agg('count')\n",
    "    # Number of properties in the city\n",
    "    df['N-city_count'] = df['regionidcity'].agg('count')\n",
    "    # Does property have a garage, pool or hot tub and AC?\n",
    "    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n",
    "                         (df['pooltypeid10']>0) & \\\n",
    "                         (df['airconditioningtypeid']!=5))*1 \n",
    "\n",
    "    # More features\n",
    "    # Mean square feet of neighborhood properties\n",
    "    df['mean_area'] = df.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].agg('median')\n",
    "    # Median year of construction of neighborhood properties\n",
    "    df['med_year'] = df.groupby('regionidneighborhood')['yearbuilt'].agg('median')\n",
    "    # Neighborhood latitude and longitude\n",
    "    df['med_lat'] = df.groupby('regionidneighborhood')['latitude'].agg('median')\n",
    "    df['med_long'] = df.groupby('regionidneighborhood')['longitude'].agg('median')\n",
    "\n",
    "    df['zip_std'] = df['regionidzip'].map(zipstd)\n",
    "    df['city_std'] = df['regionidcity'].map(citystd)\n",
    "    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)\n",
    "    \n",
    "    if runDetails['USE_SEASONAL_FEATURES']:\n",
    "        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "                             (2*np.pi/365.25) ).apply(np.cos)\n",
    "        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "                             (2*np.pi/365.25) ).apply(np.sin)  \n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class FeatureSummariser(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, subset, agg):\n",
    "        \n",
    "        self.subset = subset\n",
    "        self.agg = agg\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        \n",
    "        \n",
    "        \n",
    "        return X * self.factor\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropvars = ['parcelid', 'airconditioningtypeid', 'buildingclasstypeid',\n",
    "            'buildingqualitytypeid', 'regionidcity']\n",
    "droptrain = ['logerror', 'transactiondate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-f20c6998ebc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropvars\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdroptrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselect_qtr4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"logerror\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselect_qtr4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = calculate_features(train_df)\n",
    "\n",
    "x_valid = train_df.drop(dropvars+droptrain, axis=1)[select_qtr4]\n",
    "y_valid = train_df[\"logerror\"].values.astype(np.float32)[select_qtr4]\n",
    "\n",
    "print('Shape full training set: {}'.format(train_df.shape))\n",
    "print('Dropped vars: {}'.format(len(dropvars+droptrain)))\n",
    "print('Shape valid X: {}'.format(x_valid.shape))\n",
    "print('Shape valid y: {}'.format(y_valid.shape))\n",
    "\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "print('\\nFull training set after removing outliers, before dropping vars:')     \n",
    "print('Shape training set: {}\\n'.format(train_df.shape))\n",
    "\n",
    "if runDetails['FIT_FULL_TRAIN_SET']:\n",
    "    full_train = train_df.copy()\n",
    "\n",
    "train_df=train_df[~select_qtr4]\n",
    "x_train=train_df.drop(dropvars+droptrain, axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "n_train = x_train.shape[0]\n",
    "print('Training subset after removing outliers:')     \n",
    "print('Shape train X: {}'.format(x_train.shape))\n",
    "print('Shape train y: {}'.format(y_train.shape))\n",
    "\n",
    "if runDetails['FIT_FULL_TRAIN_SET']:\n",
    "    x_full = full_train.drop(dropvars+droptrain, axis=1)\n",
    "    y_full = full_train[\"logerror\"].values.astype(np.float32)\n",
    "    n_full = x_full.shape[0]\n",
    "    print('\\nFull trainng set:')     \n",
    "    print('Shape train X: {}'.format(x_train.shape))\n",
    "    print('Shape train y: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape test: (2985217, 65)\n"
     ]
    }
   ],
   "source": [
    "if not runDetails['CV_ONLY']:\n",
    "    test_df = properties.copy()\n",
    "    droptest = []\n",
    "    if runDetails['USE_SEASONAL_FEATURES']:\n",
    "        test_df['transactiondate'] = '2016-10-31'\n",
    "        droptest = ['transactiondate']\n",
    "    calculate_features(test_df)\n",
    "    x_test = test_df.drop(dropvars+droptest, axis=1)\n",
    "    print('Shape test: {}'.format(x_test.shape))\n",
    "\n",
    "    # Process properties for 2017\n",
    "    properties = properties17\n",
    "    for c in properties.columns:\n",
    "        properties[c]=properties[c].fillna(-1)\n",
    "        if properties[c].dtype == 'object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(properties[c].values))\n",
    "            properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "    test_df = properties.copy()\n",
    "    if runDetails['USE_SEASONAL_FEATURES']:\n",
    "        test_df['transactiondate'] = '2017-10-31'\n",
    "    test_df = calculate_features(test_df)\n",
    "    x_test17 = test_df.drop(dropvars+droptest, axis=1)   \n",
    "    del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df\n",
    "del select_qtr4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {  # best as of 2017-09-28 13:20 UTC\n",
    "    'eta': modelDesc['LEARNING_RATE'],\n",
    "    'max_depth': 7, \n",
    "    'subsample': 0.6,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 5.0,\n",
    "    'alpha': 0.65,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'base_score': y_mean,'taxdelinquencyyear'\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data into a xgboost friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dvalid_x = xgb.DMatrix(x_valid)\n",
    "dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n",
    "\n",
    "if not runDetails['CV_ONLY']:\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "    dtest17 = xgb.DMatrix(x_test17)\n",
    "    del x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting rounds: 2857\n",
      "Early stoping rounds: 143\n"
     ]
    }
   ],
   "source": [
    "num_boost_rounds = round( modelDesc['ROUNDS_PER_ETA'] / xgb_params['eta'] )\n",
    "early_stopping_rounds = round( num_boost_rounds / 20 )\n",
    "print('Boosting rounds: {}'.format(num_boost_rounds))\n",
    "print('Early stoping rounds: {}'.format(early_stopping_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.053445\teval-mae:0.065272\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 143 rounds.\n",
      "[10]\ttrain-mae:0.053359\teval-mae:0.065206\n",
      "[20]\ttrain-mae:0.053273\teval-mae:0.065138\n",
      "[30]\ttrain-mae:0.053193\teval-mae:0.065084\n",
      "[40]\ttrain-mae:0.053112\teval-mae:0.065028\n",
      "[50]\ttrain-mae:0.053035\teval-mae:0.064978\n",
      "[60]\ttrain-mae:0.05296\teval-mae:0.064928\n",
      "[70]\ttrain-mae:0.052894\teval-mae:0.064886\n",
      "[80]\ttrain-mae:0.052834\teval-mae:0.064848\n",
      "[90]\ttrain-mae:0.052778\teval-mae:0.064813\n",
      "[100]\ttrain-mae:0.05273\teval-mae:0.064784\n",
      "[110]\ttrain-mae:0.052672\teval-mae:0.064748\n",
      "[120]\ttrain-mae:0.052624\teval-mae:0.06472\n",
      "[130]\ttrain-mae:0.052578\teval-mae:0.064699\n",
      "[140]\ttrain-mae:0.052534\teval-mae:0.064676\n",
      "[150]\ttrain-mae:0.052488\teval-mae:0.064649\n",
      "[160]\ttrain-mae:0.052442\teval-mae:0.064626\n",
      "[170]\ttrain-mae:0.052396\teval-mae:0.064605\n",
      "[180]\ttrain-mae:0.052357\teval-mae:0.064585\n",
      "[190]\ttrain-mae:0.052323\teval-mae:0.064568\n",
      "[200]\ttrain-mae:0.052283\teval-mae:0.064555\n",
      "[210]\ttrain-mae:0.052247\teval-mae:0.064541\n",
      "[220]\ttrain-mae:0.052212\teval-mae:0.064528\n",
      "[230]\ttrain-mae:0.052177\teval-mae:0.064515\n",
      "[240]\ttrain-mae:0.052145\teval-mae:0.064502\n",
      "[250]\ttrain-mae:0.05211\teval-mae:0.064488\n",
      "[260]\ttrain-mae:0.052079\teval-mae:0.064478\n",
      "[270]\ttrain-mae:0.052046\teval-mae:0.064465\n",
      "[280]\ttrain-mae:0.052014\teval-mae:0.064451\n",
      "[290]\ttrain-mae:0.051987\teval-mae:0.064445\n",
      "[300]\ttrain-mae:0.051954\teval-mae:0.064436\n",
      "[310]\ttrain-mae:0.05193\teval-mae:0.064425\n",
      "[320]\ttrain-mae:0.051905\teval-mae:0.064419\n",
      "[330]\ttrain-mae:0.051878\teval-mae:0.064413\n",
      "[340]\ttrain-mae:0.051856\teval-mae:0.064405\n",
      "[350]\ttrain-mae:0.051828\teval-mae:0.064398\n",
      "[360]\ttrain-mae:0.051802\teval-mae:0.064397\n",
      "[370]\ttrain-mae:0.051776\teval-mae:0.064388\n",
      "[380]\ttrain-mae:0.051748\teval-mae:0.064384\n",
      "[390]\ttrain-mae:0.051726\teval-mae:0.064381\n",
      "[400]\ttrain-mae:0.051705\teval-mae:0.064378\n",
      "[410]\ttrain-mae:0.05168\teval-mae:0.064374\n",
      "[420]\ttrain-mae:0.051655\teval-mae:0.064371\n",
      "[430]\ttrain-mae:0.051628\teval-mae:0.064364\n",
      "[440]\ttrain-mae:0.051609\teval-mae:0.064361\n",
      "[450]\ttrain-mae:0.051585\teval-mae:0.06436\n",
      "[460]\ttrain-mae:0.051566\teval-mae:0.064358\n",
      "[470]\ttrain-mae:0.051545\teval-mae:0.064354\n",
      "[480]\ttrain-mae:0.051522\teval-mae:0.06435\n",
      "[490]\ttrain-mae:0.0515\teval-mae:0.064349\n",
      "[500]\ttrain-mae:0.05148\teval-mae:0.064346\n",
      "[510]\ttrain-mae:0.051459\teval-mae:0.064343\n",
      "[520]\ttrain-mae:0.05144\teval-mae:0.06434\n",
      "[530]\ttrain-mae:0.05142\teval-mae:0.06434\n",
      "[540]\ttrain-mae:0.051397\teval-mae:0.064339\n",
      "[550]\ttrain-mae:0.051376\teval-mae:0.064337\n",
      "[560]\ttrain-mae:0.051354\teval-mae:0.064333\n",
      "[570]\ttrain-mae:0.051332\teval-mae:0.06433\n",
      "[580]\ttrain-mae:0.051312\teval-mae:0.064328\n",
      "[590]\ttrain-mae:0.051293\teval-mae:0.064327\n",
      "[600]\ttrain-mae:0.051274\teval-mae:0.064326\n",
      "[610]\ttrain-mae:0.051255\teval-mae:0.064326\n",
      "[620]\ttrain-mae:0.051235\teval-mae:0.064323\n",
      "[630]\ttrain-mae:0.051215\teval-mae:0.064321\n",
      "[640]\ttrain-mae:0.051196\teval-mae:0.064317\n",
      "[650]\ttrain-mae:0.051177\teval-mae:0.064314\n",
      "[660]\ttrain-mae:0.051158\teval-mae:0.064314\n",
      "[670]\ttrain-mae:0.05114\teval-mae:0.064316\n",
      "[680]\ttrain-mae:0.051124\teval-mae:0.064316\n",
      "[690]\ttrain-mae:0.051106\teval-mae:0.064316\n",
      "[700]\ttrain-mae:0.05109\teval-mae:0.064316\n",
      "[710]\ttrain-mae:0.051073\teval-mae:0.064315\n",
      "[720]\ttrain-mae:0.051059\teval-mae:0.064317\n",
      "[730]\ttrain-mae:0.05104\teval-mae:0.064315\n",
      "[740]\ttrain-mae:0.051021\teval-mae:0.064317\n",
      "[750]\ttrain-mae:0.051001\teval-mae:0.064315\n",
      "[760]\ttrain-mae:0.050982\teval-mae:0.064315\n",
      "[770]\ttrain-mae:0.050965\teval-mae:0.064315\n",
      "[780]\ttrain-mae:0.050948\teval-mae:0.064317\n",
      "[790]\ttrain-mae:0.050926\teval-mae:0.064317\n",
      "Stopping. Best iteration:\n",
      "[651]\ttrain-mae:0.051175\teval-mae:0.064313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n",
    "model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n",
    "                  evals=evals, early_stopping_rounds=early_stopping_rounds, \n",
    "                  verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost validation set predictions:\n",
      "          0\n",
      "0  0.001453\n",
      "1  0.022785\n",
      "2  0.020948\n",
      "3  0.012848\n",
      "4  0.018639\n",
      "\n",
      "Mean absolute validation error:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.064313427"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n",
    "print( \"XGBoost validation set predictions:\" )\n",
    "print( pd.DataFrame(valid_pred).head() )\n",
    "print(\"\\nMean absolute validation error:\")\n",
    "mean_absolute_error(y_valid, valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAD Fit for Fudge Factor:\n",
      "                         QuantReg Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Pseudo R-squared:              0.01241\n",
      "Model:                       QuantReg   Bandwidth:                    0.009326\n",
      "Method:                 Least Squares   Sparsity:                       0.1035\n",
      "Date:                Sun, 08 Oct 2017   No. Observations:                14304\n",
      "Time:                        11:58:21   Df Residuals:                    14303\n",
      "                                        Df Model:                            1\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.0030      0.029     34.898      0.000       0.947       1.059\n",
      "==============================================================================\n",
      "Optimized fudge factor: 1.00304\n",
      "\n",
      "Mean absolute validation error with optimized fudge factor: \n",
      "0.0643134\n",
      "Scaled down fudge factor: 1.00091240994\n",
      "\n",
      "Mean absolute validation error with scaled down fudge factor: \n",
      "0.0643134\n"
     ]
    }
   ],
   "source": [
    "if runDetails['OPTIMIZE_FUDGE_FACTOR']:\n",
    "    mod = QuantReg(y_valid, valid_pred)\n",
    "    res = mod.fit(q=.5)\n",
    "    print(\"\\nLAD Fit for Fudge Factor:\")\n",
    "    print(res.summary())\n",
    "\n",
    "    fudge = res.params[0]\n",
    "    print(\"Optimized fudge factor:\", fudge)\n",
    "    print(\"\\nMean absolute validation error with optimized fudge factor: \")\n",
    "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n",
    "\n",
    "    fudge **= runDetails['FUDGE_FACTOR_SCALEDOWN']\n",
    "    print(\"Scaled down fudge factor:\", fudge)\n",
    "    print(\"\\nMean absolute validation error with scaled down fudge factor: \")\n",
    "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n",
    "else:\n",
    "    fudge=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.053266\n",
      "[10]\ttrain-mae:0.053178\n",
      "[20]\ttrain-mae:0.053096\n",
      "[30]\ttrain-mae:0.053014\n",
      "[40]\ttrain-mae:0.05294\n",
      "[50]\ttrain-mae:0.052866\n",
      "[60]\ttrain-mae:0.0528\n",
      "[70]\ttrain-mae:0.052739\n",
      "[80]\ttrain-mae:0.052684\n",
      "[90]\ttrain-mae:0.05263\n",
      "[100]\ttrain-mae:0.052579\n",
      "[110]\ttrain-mae:0.052528\n",
      "[120]\ttrain-mae:0.05248\n",
      "[130]\ttrain-mae:0.052433\n",
      "[140]\ttrain-mae:0.052386\n",
      "[150]\ttrain-mae:0.05234\n",
      "[160]\ttrain-mae:0.0523\n",
      "[170]\ttrain-mae:0.052257\n",
      "[180]\ttrain-mae:0.052221\n",
      "[190]\ttrain-mae:0.052186\n",
      "[200]\ttrain-mae:0.052151\n",
      "[210]\ttrain-mae:0.052119\n",
      "[220]\ttrain-mae:0.052082\n",
      "[230]\ttrain-mae:0.05205\n",
      "[240]\ttrain-mae:0.052018\n",
      "[250]\ttrain-mae:0.051988\n",
      "[260]\ttrain-mae:0.051958\n",
      "[270]\ttrain-mae:0.051929\n",
      "[280]\ttrain-mae:0.051901\n",
      "[290]\ttrain-mae:0.051874\n",
      "[300]\ttrain-mae:0.051844\n",
      "[310]\ttrain-mae:0.051818\n",
      "[320]\ttrain-mae:0.051791\n",
      "[330]\ttrain-mae:0.051767\n",
      "[340]\ttrain-mae:0.051745\n",
      "[350]\ttrain-mae:0.05172\n",
      "[360]\ttrain-mae:0.051695\n",
      "[370]\ttrain-mae:0.051671\n",
      "[380]\ttrain-mae:0.051648\n",
      "[390]\ttrain-mae:0.051624\n",
      "[400]\ttrain-mae:0.051602\n",
      "[410]\ttrain-mae:0.051579\n",
      "[420]\ttrain-mae:0.051555\n",
      "[430]\ttrain-mae:0.051534\n",
      "[440]\ttrain-mae:0.051512\n",
      "[450]\ttrain-mae:0.051492\n",
      "[460]\ttrain-mae:0.051472\n",
      "[470]\ttrain-mae:0.051454\n",
      "[480]\ttrain-mae:0.051433\n",
      "[490]\ttrain-mae:0.051414\n",
      "[500]\ttrain-mae:0.051391\n",
      "[510]\ttrain-mae:0.051376\n",
      "[520]\ttrain-mae:0.051357\n",
      "[530]\ttrain-mae:0.051336\n",
      "[540]\ttrain-mae:0.051316\n",
      "[550]\ttrain-mae:0.051299\n",
      "[560]\ttrain-mae:0.051281\n",
      "[570]\ttrain-mae:0.051264\n",
      "[580]\ttrain-mae:0.051247\n",
      "[590]\ttrain-mae:0.051226\n",
      "[600]\ttrain-mae:0.051207\n",
      "[610]\ttrain-mae:0.051192\n",
      "[620]\ttrain-mae:0.051174\n",
      "[630]\ttrain-mae:0.051158\n",
      "[640]\ttrain-mae:0.051141\n",
      "[650]\ttrain-mae:0.051125\n",
      "[660]\ttrain-mae:0.051108\n",
      "[670]\ttrain-mae:0.051093\n",
      "[680]\ttrain-mae:0.051079\n",
      "[690]\ttrain-mae:0.051064\n",
      "[700]\ttrain-mae:0.051046\n",
      "[710]\ttrain-mae:0.051029\n",
      "[720]\ttrain-mae:0.05101\n",
      "[730]\ttrain-mae:0.050997\n",
      "[740]\ttrain-mae:0.05098\n",
      "[750]\ttrain-mae:0.050966\n",
      "[760]\ttrain-mae:0.05095\n",
      "[770]\ttrain-mae:0.050934\n"
     ]
    }
   ],
   "source": [
    "if runDetails['FIT_FULL_TRAIN_SET'] and not runDetails['CV_ONLY']:\n",
    "    if runDetails['FIT_2017_TRAIN_SET']:\n",
    "        \n",
    "        train = pd.read_csv('train_2017.csv')\n",
    "        train_df = train.merge(properties, how='left', on='parcelid')\n",
    "        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n",
    "        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n",
    "        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n",
    "        train_df = calculate_features(train_df)\n",
    "        x_full = train_df.drop(dropvars+droptrain, axis=1)\n",
    "        y_full = train_df[\"logerror\"].values.astype(np.float32)\n",
    "        n_full = x_full.shape[0]     \n",
    "        \n",
    "    dtrain = xgb.DMatrix(x_full, y_full)\n",
    "    num_boost_rounds = int(model.best_ntree_limit*n_full/n_train)\n",
    "    full_model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds, \n",
    "                           evals=[(dtrain,'train')], verbose_eval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
