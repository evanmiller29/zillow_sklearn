{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating some ML pipelines\n",
    "\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought I'd do a quick write up on how you can build some simple and effective ML pipelines using sklearn.\n",
    "\n",
    "I found discovered the pipeline/gridsearch combo a few weeks ago after sending off some of my code to review.\n",
    "In sending off my code I realized that were a few things that I had tweaked for performance, but weren't obvious to the reviewer.\n",
    "\n",
    "- I had median imputed some variables (continuous features), while other variables were filled by mode\n",
    "- I did a large amount of feature engineering, only to use a subset of those features in my model building (they were the best I swear)\n",
    "\n",
    "So even though I did some work to get to that reviewed copy, these experiments that I went through during the process wouldn't be easy to understand unless the reviewer at all my revisions etc.\n",
    "\n",
    "But then I found **pipelines / gridsearch** and all was good in the world.\n",
    "\n",
    "**The pipelines let you combine all your feature engineering / pre-processing / modelling into one object**\n",
    "**Gridsearch then lets you test all your assumptions / hyperparameters to find out which combinations generate the best result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't seen many write ups about them so I thought I'd do one myself.\n",
    "\n",
    "References:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "- http://scikit-learn.org/stable/modules/pipeline.html\n",
    "- http://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
    "- https://stackoverflow.com/questions/33091376/python-what-is-exactly-sklearn-pipeline-pipeline\n",
    "- http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "- https://michelleful.github.io/code-blog/2015/06/20/pipelines/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries + MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def MAE(y, ypred):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data read in and prep\n",
    "### Making sure the data is in a lightGBM friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basePath = 'C:/Users/Evan/Documents/GitHub/Zillow'\n",
    "funcPath = 'C:/Users/Evan/Documents/GitHub/zillow_sklearn/Python'\n",
    "subPath = 'F:/Nerdy Stuff/Kaggle submissions/Zillow'\n",
    "\n",
    "os.chdir(basePath)\n",
    "\n",
    "train = pd.read_csv('data/train_2016_v2.csv')\n",
    "properties = pd.read_csv('data/properties_2016.csv', low_memory=False)\n",
    "sample = (pd.read_csv('data/sample_submission.csv')\n",
    "            .rename(columns = {'ParcelId':'parcelid'}))\n",
    "\n",
    "for c, dtype in zip(properties.columns, properties.dtypes):\t\n",
    "    if dtype == np.float64:\n",
    "        properties[c] = properties[c].astype(np.float32)\n",
    "\n",
    "df_train = (train.merge(properties, how='left', on='parcelid')\n",
    "            .drop(['parcelid', 'transactiondate', 'propertyzoningdesc', \n",
    "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1))\n",
    "\n",
    "train_columns = df_train.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset to train/valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = df_train.iloc[1:20000, :]\n",
    "train = df_train.iloc[20001:90275, :]\n",
    "\n",
    "y_train = train['logerror'].values\n",
    "y_valid = valid['logerror'].values\n",
    "\n",
    "x_train = train.drop('logerror', axis = 1)\n",
    "x_valid = valid.drop('logerror', axis = 1)\n",
    "\n",
    "idVars = [i for e in ['id',  'flag', 'has'] for i in list(train_columns) if e in i] + ['fips', 'hashottuborspa']\n",
    "countVars = [i for e in ['cnt',  'year', 'nbr', 'number'] for i in list(train_columns) if e in i]\n",
    "taxVars = [col for col in train_columns if 'tax' in col and 'flag' not in col]\n",
    "          \n",
    "ttlVars = idVars + countVars + taxVars\n",
    "dropVars = [i for e in ['census',  'tude', 'error'] for i in list(train_columns) if e in i]\n",
    "contVars = [col for col in train_columns if col not in ttlVars + dropVars]\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "    \n",
    "for c in x_valid.dtypes[x_valid.dtypes == object].index.values:\n",
    "    x_valid[c] = (x_valid[c] == True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first pipeline\n",
    "\n",
    "Since everyone is using lightGBM I'll use that. \n",
    "Initially we'll just look at the continuous variables in model building, but we'll extend that out too.\n",
    "\n",
    "So let's start with the easy pipeline that:\n",
    "\n",
    "- Imputes the missing values with the median\n",
    "- Selects the best 5 features\n",
    "- Builds a LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['basementsqft', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6', 'garagetotalsqft', 'lotsizesquarefeet', 'poolsizesum', 'yardbuildingsqft17', 'yardbuildingsqft26']\n"
     ]
    }
   ],
   "source": [
    "print(contVars)\n",
    "\n",
    "x_train_cont = x_train[contVars]\n",
    "x_valid_cont = x_valid[contVars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on validation set: 0.0735\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "                    [('imp', Imputer(missing_values='NaN', strategy = 'median', axis=0)),\n",
    "                     ('feat_select', SelectKBest(k = 5)),\n",
    "                     ('lgbm', LGBMRegressor())\n",
    "                     \n",
    "])\n",
    "\n",
    "pipeline.fit(x_train_cont, y_train)   \n",
    "\n",
    "y_pred = pipeline.predict(x_valid_cont)\n",
    "print('MAE on validation set: %s' % (round(MAE(y_valid, y_pred), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2.0 - oh hai there gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But from the above code we have made a few assumptions that haven't been tested.\n",
    "\n",
    "**We assume that:**\n",
    "- Median is the best way of imputing the variables\n",
    "- Only 5 variables needed for the lowest error \n",
    "\n",
    "But we don't need to assume these, we can test these assumptions and find out which actually results in the lowest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score and parameter combination = \n",
      "-0.0669848314136\n",
      "{'feat_select__k': 5, 'imp__strategy': 'most_frequent'}\n",
      "MAE on validation set: 0.0735\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "                    [('imp', Imputer(missing_values='NaN', axis=0)),\n",
    "                     ('feat_select', SelectKBest()),\n",
    "                     ('lgbm', LGBMRegressor())\n",
    "                     \n",
    "])\n",
    "\n",
    "parameters = {}\n",
    "parameters['imp__strategy'] = ['mean', 'median', 'most_frequent']\n",
    "parameters['feat_select__k'] = [5, 10]\n",
    "\n",
    "CV = GridSearchCV(pipeline, parameters, scoring = 'mean_absolute_error', n_jobs= 1)\n",
    "CV.fit(x_train_cont, y_train)   \n",
    "\n",
    "print('Best score and parameter combination = ')\n",
    "\n",
    "print(CV.best_score_)    \n",
    "print(CV.best_params_)    \n",
    "\n",
    "y_pred = CV.predict(x_valid_cont)\n",
    "print('MAE on validation set: %s' % (round(MAE(y_valid, y_pred), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, I never thought that using a mode would come out on top.\n",
    "\n",
    "But we since we're also here we let's also test and see what is the best imputation policy for tax variables. First I'll quickly write a column extractor that plays nicely with the pipeline.\n",
    "\n",
    "These can look hard at first, but they definitely get easier as you write a few. \n",
    "And since we're writing some code we should probably do some testing to make sure that it works the way we think it will.\n",
    "\n",
    "# Column extractor\n",
    "\n",
    "## Takes a list of columns and returns a df with those cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, subset):\n",
    "        self.subset = subset\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        return X.loc[:, self.subset]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Since we've already created x_train_cont I'll test the column extractor on this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contExtractor = ColumnExtractor(contVars)\n",
    "x_train_cont_test = contExtractor.transform(x_train).head()\n",
    "\n",
    "x_train_cont.head().equals(x_train_cont_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 3.0 - taxes\n",
    "\n",
    "So let's use the ColumnExtractor we created earlier to run the same analysis we did earlier on the tax variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feat_select__k': 5, 'imp__strategy': 'median'}\n",
      "-0.0669709337954\n",
      "MAE on validation set: 0.07356\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                    ('tax', ColumnExtractor(taxVars)),\n",
    "                    ('imp', Imputer(missing_values='NaN', axis=0)),\n",
    "                    ('feat_select', SelectKBest()),\n",
    "                    ('lgbm', LGBMRegressor())\n",
    "                     \n",
    "])\n",
    "\n",
    "parameters = dict(imp__strategy=['mean', 'median', 'most_frequent'],\n",
    "                    feat_select__k=[5, 2, 1] \n",
    "\n",
    ")   \n",
    "\n",
    "CV = GridSearchCV(pipeline, parameters, scoring = 'neg_mean_absolute_error', n_jobs= 1)\n",
    "CV.fit(x_train, y_train)   \n",
    "\n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)    \n",
    "\n",
    "y_pred = CV.predict(x_valid)\n",
    "print('MAE on validation set: %s' % (round(MAE(y_valid, y_pred), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 4.0 - contVars + taxes (FeatureUnion intro)\n",
    "\n",
    "To come"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
